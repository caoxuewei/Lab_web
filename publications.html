<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>学术成果 - SPIN Lab</title>
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/style.css">
  <!-- Font Awesome 图标库 -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <!-- 谷歌字体 -->
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
  <style>
    /* 学术成果页特有样式 */
    .breadcrumb {
      list-style: none;
      display: flex;
      justify-content: center;
      padding: 0;
      margin: 2rem 0 0;
    }

    .breadcrumb li {
      display: flex;
      align-items: center;
    }

    .breadcrumb li:not(:last-child)::after {
      content: '\f054';
      font-family: 'Font Awesome 5 Free';
      font-weight: 900;
      margin: 0 10px;
      font-size: 0.8rem;
      color: var(--dark-gray);
    }

    .publications-container {
      padding: 60px 0;
    }

    .publications-filter {
      display: flex;
      justify-content: center;
      margin-bottom: 40px;
      flex-wrap: wrap;
      gap: 15px;
    }

    .filter-btn {
      padding: 8px 20px;
      border: none;
      background-color: var(--light-gray);
      border-radius: 30px;
      cursor: pointer;
      font-weight: 500;
      transition: all 0.3s ease;
    }

    .filter-btn:hover {
      background-color: var(--medium-gray);
    }

    .filter-btn.active {
      background-color: var(--primary-color);
      color: white;
    }

    .publications-year {
      margin-bottom: 50px;
    }

    .year-heading {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }

    .year-heading h2 {
      font-size: 1.8rem;
      margin: 0;
      position: relative;
      display: inline-block;
      padding-right: 20px;
    }

    .year-line {
      flex: 1;
      height: 2px;
      background-color: var(--medium-gray);
    }

    .publication-card {
      background-color: white;
      border-radius: var(--card-radius);
      padding: 25px;
      margin-bottom: 25px;
      box-shadow: 0 5px 20px rgba(0, 0, 0, 0.05);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .publication-card:hover {
      transform: translateY(-5px);
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
    }

    .publication-title {
      font-size: 1.2rem;
      margin: 0 0 15px;
      color: var(--dark-color);
    }

    .publication-authors {
      margin-bottom: 10px;
      color: var(--dark-gray);
      font-size: 0.95rem;
    }

    .publication-venue {
      display: inline-block;
      margin-bottom: 15px;
      font-weight: 500;
      color: var(--primary-color);
      padding: 3px 12px;
      border-radius: 20px;
      font-size: 0.85rem;
      background-color: rgba(var(--primary-rgb), 0.1);
    }

    .publication-abstract {
      margin-bottom: 20px;
      color: var(--dark-gray);
      font-size: 0.95rem;
      line-height: 1.6;
    }

    .publication-links {
      display: flex;
      flex-wrap: wrap;
      gap: 15px;
    }

    .publication-link {
      display: inline-flex;
      align-items: center;
      padding: 5px 15px;
      border-radius: 20px;
      background-color: var(--light-gray);
      color: var(--dark-gray);
      font-size: 0.85rem;
      text-decoration: none;
      transition: all 0.3s ease;
    }

    .publication-link:hover {
      background-color: var(--primary-color);
      color: white;
    }

    .publication-link i {
      margin-right: 5px;
    }

    .publication-info {
      margin-top: 15px;
      display: flex;
      align-items: center;
      justify-content: space-between;
      font-size: 0.85rem;
    }

    .publication-tags {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
    }

    .publication-tag {
      display: inline-block;
      padding: 3px 10px;
      border-radius: 20px;
      background-color: var(--light-gray);
      color: var(--dark-gray);
    }

    .publication-metrics {
      display: flex;
      gap: 15px;
    }

    .metric {
      display: flex;
      align-items: center;
      gap: 5px;
    }

    .citation-count {
      color: var(--primary-color);
      font-weight: 600;
    }

    .publications-nav {
      text-align: center;
      margin-top: 50px;
    }

    .publications-nav button {
      background-color: var(--light-gray);
      border: none;
      padding: 10px 20px;
      margin: 0 5px;
      border-radius: var(--button-radius);
      cursor: pointer;
      transition: all 0.3s ease;
    }

    .publications-nav button:hover,
    .publications-nav button.active {
      background-color: var(--primary-color);
      color: white;
    }

    .load-more {
      display: block;
      width: 200px;
      margin: 40px auto 0;
      padding: 12px 25px;
      text-align: center;
      background-color: var(--primary-color);
      color: white;
      border: none;
      border-radius: var(--button-radius);
      cursor: pointer;
      transition: all 0.3s ease;
    }

    .load-more:hover {
      background-color: var(--accent-color);
    }

    @media (max-width: 768px) {
      .publication-info {
        flex-direction: column;
        align-items: flex-start;
      }

      .publication-metrics {
        margin-top: 10px;
      }

      .publications-filter {
        justify-content: flex-start;
        overflow-x: auto;
        padding-bottom: 10px;
      }
    }
  </style>
</head>

<body>
  <!-- 导航栏 -->
  <header class="header">
    <div class="container">
      <div class="logo">
        <a href="index.html">
          <h1>中科大空间智能实验室 SPIN Lab</h1>
        </a>
      </div>
      <div class="nav-right">
        <nav class="main-nav">
          <ul>
            <li><a href="index.html">首页</a></li>
            <li><a href="about.html">实验室简介</a></li>

            <li><a href="team.html">团队成员</a></li>
            <li><a href="publications.html" class="active">学术成果</a></li>
            <li><a href="admission.html">招生信息</a></li>
          </ul>
        </nav>
        <div class="search-box">
          <input type="text" placeholder="搜索...">
          <button type="submit"><i class="fas fa-search"></i></button>
        </div>
      </div>
      <div class="menu-toggle">
        <i class="fas fa-bars"></i>
      </div>
    </div>
  </header>

  <!-- 论文列表 -->
  <section class="publications-container" style="padding-top: 140px;">
    <div class="container">
      <!-- 过滤器 -->
      <div class="publications-filter">
        <button class="filter-btn active" data-filter="all">全部成果</button>

        <button class="filter-btn" data-filter="conference">会议论文</button>

        <button class="filter-btn" data-filter="journal">期刊论文</button>
      </div>

      <!-- 2025年 -->
      <div class="publications-year">
        <div class="year-heading">
          <h2>2025</h2>
          <div class="year-line"></div>
        </div>

        <!-- 论文1: SparseAlign -->
        <div class="publication-card" data-category="conference">
          <h3 class="publication-title">SparseAlign: A Fully Sparse Framework for Cooperative Object Detection</h3>
          <p class="publication-authors">Yunshuang Yuan, Yan Xia†, Daniel Cremers, Monika Sester</p>
          <span class="publication-venue">CVPR 2025</span>
          <p class="publication-abstract">
            This paper proposes SparseAlign, a fully sparse framework for cooperative object detection. The framework
            achieves efficient multi-agent cooperative perception through sparse feature representation and alignment
            mechanisms. Our method significantly reduces communication overhead while maintaining detection accuracy,
            providing a novel solution for cooperative perception in autonomous driving and intelligent transportation
            systems.
          </p>
          <div class="publication-links">
            <a href="https://arxiv.org/pdf/2503.12982" class="publication-link"><i class="fas fa-file-pdf"></i>
              论文PDF</a>
            <a href="#" class="https://arxiv.org/pdf/2503.12982"><i class="fab fa-github"></i> 代码</a>
            <a href="#" class="https://arxiv.org/pdf/2503.12982"><i class="fas fa-external-link-alt"></i> DOI</a>
          </div>
          <div class="publication-info">
            <div class="publication-tags">
              <span class="publication-tag">协同目标检测</span>
              <span class="publication-tag">全稀疏框架</span>
              <span class="publication-tag">自动驾驶</span>
            </div>
            <div class="publication-metrics">
              <div class="metric">
                <i class="fas fa-quote-right"></i>
                <span class="citation-count">-</span>
              </div>
            </div>
          </div>
        </div>

        <!-- 论文2: Localizing Events -->
        <div class="publication-card" data-category="conference">
          <h3 class="publication-title">Localizing Events in Videos with Multimodal Queries</h3>
          <p class="publication-authors">Gengyuan Zhang, Mang Ling Ada Fok, Jialu Ma, Yan Xia†, Daniel Cremers, Philip
            Torr, Volker Tresp, Jindong Gu</p>
          <span class="publication-venue">CVPR 2025</span>
          <p class="publication-abstract">
            This paper investigates the problem of localizing events in videos using multimodal queries. We propose a
            novel approach that can handle queries from multiple modalities including text, images, and audio to
            accurately localize relevant events in videos. Our method achieves state-of-the-art performance on multiple
            benchmark datasets, providing important contributions to the fields of video understanding and retrieval.
          </p>
          <div class="publication-links">
            <a href="https://arxiv.org/pdf/2406.10079" class="publication-link"><i class="fas fa-file-pdf"></i>
              论文PDF</a>
            <a href="https://arxiv.org/pdf/2406.10079" class="publication-link"><i class="fab fa-github"></i> 代码</a>
            <a href="https://arxiv.org/pdf/2406.10079" class="publication-link"><i class="fas fa-external-link-alt"></i>
              DOI</a>
          </div>
          <div class="publication-info">
            <div class="publication-tags">
              <span class="publication-tag">多模态查询</span>
              <span class="publication-tag">视频事件定位</span>
              <span class="publication-tag">视频理解</span>
            </div>
            <div class="publication-metrics">
              <div class="metric">
                <i class="fas fa-quote-right"></i>
                <span class="citation-count">-</span>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- 2024年 -->
      <div class="publications-year">
        <div class="year-heading">
          <h2>2024</h2>
          <div class="year-line"></div>
        </div>

        <!-- 论文1: Text2Loc -->
        <div class="publication-card" data-category="conference">
          <h3 class="publication-title">Text2Loc: 3D Point Cloud Localization from Natural Language</h3>
          <p class="publication-authors">Yan Xia*†, Letian Shi*, Zifeng Ding, João F. Henriques, Daniel Cremers</p>
          <span class="publication-venue">CVPR 2024</span>
          <p class="publication-abstract">
            This paper presents Text2Loc, a novel method for 3D point cloud localization from natural language
            descriptions. We design a hierarchical Transformer architecture that can understand natural language
            descriptions and accurately localize target positions in large-scale 3D point cloud scenes. Our method
            achieves excellent performance on multiple indoor and outdoor datasets, opening new research directions for
            language-based 3D scene understanding.
          </p>
          <div class="publication-links">
            <a href="https://arxiv.org/pdf/2311.15977" class="publication-link"><i class="fas fa-file-pdf"></i>
              论文PDF</a>
            <a href="https://arxiv.org/pdf/2311.15977" class="publication-link"><i class="fab fa-github"></i> 代码</a>
            <a href="https://arxiv.org/pdf/2311.15977" class="publication-link"><i class="fas fa-external-link-alt"></i>
              DOI</a>
          </div>
          <div class="publication-info">
            <div class="publication-tags">
              <span class="publication-tag">3D点云定位</span>
              <span class="publication-tag">层次化Transformer</span>
              <span class="publication-tag">自然语言处理</span>
            </div>
            <div class="publication-metrics">
              <div class="metric">
                <i class="fas fa-quote-right"></i>
                <span class="citation-count">12</span>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- 2023年 -->
      <div class="publications-year">
        <div class="year-heading">
          <h2>2023</h2>
          <div class="year-line"></div>
        </div>

        <!-- 论文1: CASSPR -->
        <div class="publication-card" data-category="conference">
          <h3 class="publication-title">CASSPR: Cross Attention Single Scan Place Recognition</h3>
          <p class="publication-authors">Yan Xia*†, Mariia Gladkova*, Rui Wang, Qianyun Li, Uwe Stilla, João F.
            Henriques, Daniel Cremers</p>
          <span class="publication-venue">ICCV 2023</span>
          <p class="publication-abstract">
            This paper proposes CASSPR, a cross attention single scan place recognition method. The approach employs an
            innovative cross attention architecture that enables efficient place recognition from single LiDAR scans,
            significantly improving recognition accuracy and robustness in complex environments. Our method achieves
            state-of-the-art performance on multiple benchmark datasets, providing important technical support for
            robotic navigation and SLAM systems.
          </p>
          <div class="publication-links">
            <a href="https://arxiv.org/pdf/2211.12542" class="publication-link"><i class="fas fa-file-pdf"></i>
              论文PDF</a>
            <a href="https://arxiv.org/pdf/2211.12542" class="publication-link"><i class="fab fa-github"></i> 代码</a>
            <a href="https://arxiv.org/pdf/2211.12542" class="publication-link"><i class="fas fa-external-link-alt"></i>
              DOI</a>
          </div>
          <div class="publication-info">
            <div class="publication-tags">
              <span class="publication-tag">地点识别</span>
              <span class="publication-tag">交叉注意力</span>
              <span class="publication-tag">激光雷达</span>
              <span class="publication-tag">机器人导航</span>
            </div>
            <div class="publication-metrics">
              <div class="metric">
                <i class="fas fa-quote-right"></i>
                <span class="citation-count">25</span>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- 2021年 -->
      <div class="publications-year">
        <div class="year-heading">
          <h2>2021</h2>
          <div class="year-line"></div>
        </div>

        <!-- 论文1: SOE-net -->
        <div class="publication-card" data-category="conference">
          <h3 class="publication-title">SOE-net: A self-attention and orientation encoding network for point cloud
            based
            place recognition</h3>
          <p class="publication-authors">Yan Xia, Yusheng Xu, Shuang Li, Rui Wang, Juan Du, Daniel Cremers, Uwe Stilla
          </p>
          <span class="publication-venue">CVPR 2021</span>
          <p class="publication-abstract">
            This paper proposes SOE-net, a self-attention and orientation encoding network for point cloud based place
            recognition. The method captures geometric structure information of point clouds through innovative
            orientation encoding techniques, and combines self-attention mechanisms to learn long-range dependencies
            between points, significantly improving point cloud-based place recognition accuracy. SOE-net achieves
            excellent performance on multiple benchmark datasets, providing important technical support for mobile
            robot
            localization and navigation.
          </p>
          <div class="publication-links">
            <a href="https://arxiv.org/pdf/2011.12430" class="publication-link"><i class="fas fa-file-pdf"></i>
              论文PDF</a>
            <a href="https://arxiv.org/pdf/2011.12430" class="publication-link"><i class="fab fa-github"></i> 代码</a>
            <a href="https://arxiv.org/pdf/2011.12430" class="publication-link"><i class="fas fa-external-link-alt"></i>
              DOI</a>
          </div>
          <div class="publication-info">
            <div class="publication-tags">
              <span class="publication-tag">地点识别</span>
              <span class="publication-tag">自注意力机制</span>
              <span class="publication-tag">方向编码</span>
              <span class="publication-tag">点云处理</span>
            </div>
            <div class="publication-metrics">
              <div class="metric">
                <i class="fas fa-quote-right"></i>
                <span class="citation-count">89</span>
              </div>
            </div>
          </div>
        </div>
      </div>

    </div>

    <!-- 分页 -->
    <div class="publications-nav">
      <button class="active">1</button>
      <button>2</button>
      <button>3</button>
      <button><i class="fas fa-ellipsis-h"></i></button>
      <button>10</button>
    </div>
    </div>
  </section>

  <!-- 页脚 -->
  <footer class="footer">
    <div class="container">
      <div class="footer-content">
        <div class="footer-column">
          <h3>关于我们</h3>
          <p>SPIN Lab致力于空间智能与机器人等领域的前沿研究，培养高水平国际化人才。</p>
        </div>
        <div class="footer-column">
          <h3>快速链接</h3>
          <ul>
            <li><a href="index.html">首页</a></li>
            <li><a href="about.html">实验室简介</a></li>

            <li><a href="team.html">团队成员</a></li>
            <li><a href="publications.html">学术成果</a></li>
          </ul>
        </div>
        <div class="footer-column">
          <h3>更多资源</h3>
          <ul>
            <li><a href="#">研究数据集</a></li>
            <li><a href="#">开源代码</a></li>
            <li><a href="#">学术资源</a></li>
            <li><a href="#">实验室内网</a></li>
          </ul>
        </div>
        <div class="footer-column">
          <h3>订阅我们</h3>
          <p>订阅我们的通讯，获取最新研究动态</p>
          <form class="subscribe-form">
            <input type="email" placeholder="您的邮箱地址">
            <button type="submit" class="btn">订阅</button>
          </form>
        </div>
      </div>
      <div class="footer-bottom">
        <p>&copy; 2025 SPIN Lab. 保留所有权利.</p>
      </div>
    </div>
  </footer>

  <!-- JavaScript -->
  <script src="js/script.js"></script>
  <script>
    // 论文过滤功能
    document.addEventListener('DOMContentLoaded', function () {
      const filterButtons = document.querySelectorAll('.filter-btn');
      const publicationCards = document.querySelectorAll('.publication-card');

      filterButtons.forEach(button => {
        button.addEventListener('click', function () {
          // 更新按钮状态
          filterButtons.forEach(btn => btn.classList.remove('active'));
          this.classList.add('active');

          // 获取过滤类别
          const filterValue = this.getAttribute('data-filter');

          // 显示/隐藏相应的论文
          publicationCards.forEach(card => {
            if (filterValue === 'all' || card.getAttribute('data-category') === filterValue) {
              card.style.display = 'block';
            } else {
              card.style.display = 'none';
            }
          });
        });
      });

      // 分页功能
      const paginationButtons = document.querySelectorAll('.publications-nav button');
      paginationButtons.forEach(button => {
        button.addEventListener('click', function () {
          paginationButtons.forEach(btn => btn.classList.remove('active'));
          this.classList.add('active');
          // 这里可以添加实际分页逻辑
        });
      });
    });
  </script>
</body>

</html>